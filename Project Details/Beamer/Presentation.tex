\documentclass{beamer}
\usetheme{Madrid}
\usepackage{xcolor}
\usecolortheme{whale}
\useoutertheme{miniframes} % Adds navigation dots

\title{Introduction to State Space Models}
\subtitle{UG BTech - 2nd Year}
\author{Vinay , Vaibhav Mahore , Shubhadeep Sing , Snehal Biswas}
\institute{Indian Institute of Science, Bangalore}
\date{\today}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Outline}
\tableofcontents
\end{frame}

\section{Introduction}

% Slide 1: Need for Sequential Data Modeling
\begin{frame}
\frametitle{The Need for Sequential Data Modeling}
\begin{itemize}
    \item \textbf{What is Sequential Data?}   \\
    \begin{itemize}
        \item \textbf{Ans :} It's the  data that comes in a specific order, where the arrangement of the data points matters.
    \end{itemize}
    \item \textbf{Example(NLP):}  
    The \textcolor{blue}{dog} bites the \textcolor{red}{man} vs The \textcolor{red}{man} bites the \textcolor{blue}{dog}.
    \item \textbf{Need for Sequential Data Modeling:}
    It's crucial because many datasets have an inherent order e.g Language, Time series in which the sequence and context between data points are essential for accurate analysis and predictions.
    \item \textbf{Challenge:}  
    Traditional models(e.g. Feedforward Networks) treat inputs independently and fail to capture such temporal dependencies.
\end{itemize}
\end{frame}

% Slide 2: RNN vs. Feedforward Neural Networks
\begin{frame}
\frametitle{Recurrent Neural Networks(RNNs)}
\begin{itemize}
    \item \textbf{RNNs are used because they:}
    \begin{itemize}
        \item Capture temporal dependencies in sequential data (e.g., time series, text, speech).
        \item Retain memory of previous inputs via hidden states, allowing context-aware predictions.
    \ite\item Address the limitation of feedforward networks, which treat inputs as independent and ignore sequence order.m Example: Predicting the next word in a sentence requires knowing prior wordsâ€”RNNs inherently model this relationship.    
    \end{itemize}
    \item \textbf{Recurrent Neural Networks (RNNs) is a class of artificial neural networks designed for sequential data (e.g., text, time series, speech). RNNs have feedback loops that allow them to maintain a "memory" of previous inputs, making them suitable for tasks where context and order matter. Unlike standard networks, they:}
    \begin{itemize}
        \item Retain memory: Use hidden states to pass information between steps in a sequence.
        \item Capture context: Model dependencies over time (e.g., predicting the next word using prior words).
        \item Struggle with long sequences: Prone to forgetting distant inputs (fixed by LSTM).
        \item Example: Used in chatbots, stock prediction, and speech recognition 
    \end{itemize}
    .
\end{itemize}
\end{frame}

% Slide 3: Transition from RNN to LSTM
\begin{frame}
\frametitle{From RNN to LSTM}
\begin{itemize}
    \item \textbf{Limitations of Vanilla RNNs:}
    \begin{itemize}
        \item Suffer from vanishing and exploding gradients during training.
        \item Struggle with capturing long-range dependencies.
    \end{itemize}
    \item \textbf{Introduction of LSTMs:}
    \begin{itemize}
        \item LSTMs introduce gating mechanisms (input, forget, output) to better manage memory.
        \item They effectively mitigate gradient issues and improve long-term dependency learning.
    \end{itemize}
\end{itemize}
\end{frame}

% Slide 4: Limitations of LSTM (with research backing)
\begin{frame}
\frametitle{Limitations of LSTMs}
\begin{itemize}
    \item \textbf{Scalability and Computational Complexity:}
    \begin{itemize}
        \item Despite improvements, LSTMs remain computationally intensive for very long sequences.
    \end{itemize}
    \item \textbf{Long-Range Dependency Challenges:}
    \begin{itemize}
        \item Research such as Bengio et al. (1994) and Pascanu et al. (2013) has demonstrated that LSTMs can still struggle with vanishing gradients when modeling very long sequences.
    \end{itemize}
    \item \textbf{Sequential Processing Bottleneck:}
    \begin{itemize}
        \item The inherent sequential nature of LSTM processing limits parallelism, resulting in slower training compared to more modern architectures.
    \end{itemize}
\end{itemize}
\end{frame}


\section{Methodology}
\begin{frame}
\frametitle{Methodology}
\begin{itemize}
    \item Data collection and preprocessing
    \item Model architecture
    \item Training approach
    \item Evaluation metrics
\end{itemize}
\end{frame}

\section{Implementation}
\begin{frame}
\frametitle{Implementation}
\begin{itemize}
    \item Technologies used
    \item Key algorithms
    \item Technical challenges
    \item Solutions implemented
\end{itemize}
\end{frame}

\section{Results}
\begin{frame}
\frametitle{Results}
\begin{itemize}
    \item Model performance
    \item Key findings
    \item Comparative analysis
    \item Visualizations
\end{itemize}
\end{frame}

\section{Future Work}
\begin{frame}
\frametitle{Future Work}
\begin{itemize}
    \item Potential improvements
    \item Scalability considerations
    \item Additional features
    \item Research directions
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Thank You}
\begin{center}
    Questions?
\end{center}
\end{frame}

\end{document}
